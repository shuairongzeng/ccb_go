# CCB Communication Testing - Solution Design

> Generated by all-plan collaborative design process (Codex-led)

## Overview

**Goal**: Build a comprehensive automated test harness that validates strict per-project isolation and communication stability across CCB ask/ping/pend/daemon workflows for all providers.

**Readiness Score**: 69/100

**Generated**: 2026-01-21

---

## Requirements Summary

### Problem Statement
CCB communication flows must be tested end-to-end across realistic scenarios (new project init, legacy restore, multi-project parallel use, stress, large payloads) while enforcing strict `.ccb_config`-anchored isolation and preventing cross-project leakage.

### Scope
In scope:
- ask/ping/pend commands for codex/gemini/opencode/claude.
- daemon lifecycle (autostart, managed shutdown, stale lock recovery).
- project lifecycle (new project, legacy migration, restore).
- file structure edge cases (nested dirs, missing `.ccb_config`, parent anchor).
- parallel and serial queueing; cancellation and blocking behavior.
- stress and large payload handling.
- mixed provider mounting and mixed invocation paths (codex + claude, etc.).

Out of scope:
- Provider CLI internal correctness beyond observed I/O contract.
- UI/terminal UX tests.
- Cross-machine or distributed IPC.

### Success Criteria
- [ ] No cross-project leakage in any test case (ask/ping/pend/daemon).
- [ ] Full automated suite runs with deterministic stubs when real CLIs are absent.
- [ ] Stress suite completes within defined throughput/latency thresholds.

### Constraints
- None specified.

### Assumptions
- Stub provider binaries and fake logs can simulate all core communication flows.
- Real provider CLIs are optional; tests skip gracefully if absent.

---

## Architecture

### Approach
Use a layered testing pyramid:
1) Unit tests for resolution, routing, and protocol parsing.
2) Integration tests with stub providers and real daemons.
3) System scenario scripts covering real-world project workflows.
4) Stress and load tests that exercise queueing and timeouts.

### Key Components
- **Project Fixture**: Creates temp project roots and `.ccb_config` anchors.
- **Stub Providers**: Fake codex/gemini/claude/opencode binaries and logs.
- **Daemon Orchestrator**: Starts/stops per-project daemons, asserts managed state.
- **Isolation Verifier**: Ensures registry/session/log routing never crosses project id.
- **Scenario Runner**: Shell scripts for complex workflows and mixed providers.

### Data Flow
Tests generate isolated project roots, start providers/daemons with stub CLIs, send ask/ping/pend, and verify results by reading per-project logs/state files. Strict project id routing is enforced throughout.

---

## Implementation Plan

### Step 1: Baseline Fixtures and Stubs
- **Actions**: Build pytest fixtures for project roots, `.ccb_config`, registry seeding, and stub provider binaries/logs.
- **Deliverables**: `test/fixtures/` (or helpers), stub CLI scripts, reusable log builders.
- **Dependencies**: None.

### Step 2: Unit + Integration Coverage Expansion
- **Actions**: Add unit tests for project id resolution, session file discovery, registry routing, and pend reader filters. Add integration tests for daemon autostart/lock/managed shutdown.
- **Deliverables**: New pytest files under `test/` and expanded coverage for edge cases.
- **Dependencies**: Step 1.

### Step 3: System Scenarios (Project Lifecycle)
- **Actions**: Extend scenario scripts to cover new project init, legacy session migration, and restore flows.
- **Deliverables**: New scripts under `test/` (e.g. `test/system_project_lifecycle.sh`).
- **Dependencies**: Step 1.

### Step 4: Multi-Project Parallelism + Mixed Providers
- **Actions**: Create scenarios with concurrent asks across multiple projects and providers; assert serialized queues and no cross-talk.
- **Deliverables**: Scripts such as `test/system_multi_project_parallel.sh`.
- **Dependencies**: Steps 1-3.

### Step 5: Stress + Large Payload Tests
- **Actions**: Add stress runner with configurable concurrency, payload size, and timeouts; include long/complex text payloads.
- **Deliverables**: `test/system_stress.sh` and a metrics report file.
- **Dependencies**: Steps 1-4.

### Step 6: CI Integration + Reporting
- **Actions**: Add a test entrypoint that runs unit/integration/system in tiers and skips live-provider tests when CLIs are missing.
- **Deliverables**: A single runner script and minimal CI hooks.
- **Dependencies**: Steps 1-5.

---

## Technical Considerations

- Strict `.ccb_config` anchoring means tests must never rely on ancestor traversal.
- Deterministic stubs should simulate ask/ping/pend contracts and produce realistic logs/state files.
- Queueing validation must measure ordering and latency without flaky sleeps (use polling with timeouts).
- tmux/WezTerm dependencies should be minimized in tests that do not require panes.
- Stress thresholds should be configurable via env vars for different machines.

---

## Risk Management

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| Timing flakiness in async tests | High | Med | Poll with timeouts; avoid fixed sleeps |
| Stub drift vs real providers | Med | Med | Periodic comparison with live CLI runs |
| Resource leaks (daemon processes) | Med | Low | Cleanup hooks + atexit sweeps |
| OS-specific path differences | Med | Med | Normalize paths and add OS-specific fixtures |

---

## Acceptance Criteria

- [ ] All ask/ping/pend flows pass for all providers with stubs and strict isolation.
- [ ] Multi-project parallel tests show zero cross-project leakage.
- [ ] Stress suite passes within defined throughput/latency thresholds.

---

## Design Contributors

| CLI | Key Contributions |
|-----|-------------------|
| Codex | Plan synthesis, concrete test layers, integration with existing scripts |
| Gemini | Test pyramid, stub provider approach, stress and chaos testing ideas |
| Claude | Not collected (user requested no wait) |
| OpenCode | Not collected (user requested no wait) |

---

## Appendix

### Clarification Summary
Readiness Score: 69/100
- Problem Clarity: 18/30 (assumption: general improvement/refactor)
- Functional Scope: 18/25
- Success Criteria: 18/20 (automated tests)
- Constraints: 10/15 (no explicit constraints)
- Priority/MVP: 5/10 (scope TBD)

### Alternative Approaches Considered
- Use live provider CLIs only: rejected due to nondeterminism and availability issues.
- Limit to unit tests: rejected because daemon IPC and registry routing require system coverage.
